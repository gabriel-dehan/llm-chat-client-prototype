# LLM Chat Client Prototype

VERY WIP, just a quick prototype for streaming chat completions to a chat client, with an hybrid approach (rendering of components inside the chat when getting JSON data from the server) 

It intefaces with a back-end using my [Gemini Completion Rails](https://github.com/gabriel-dehan/gemini-completions-rails) gem.

The idea in the future is to create a fully featured LLM chat client that handles forking the conversation elegantly
